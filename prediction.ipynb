{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9735635-9f13-4c53-b651-f5a786a4e3b7",
   "metadata": {},
   "source": [
    "`Viru`, `Temp`, `Bact` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfd7163-7e00-4bb5-bec0-7f0ae227b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "from Bio import SeqIO\n",
    "\n",
    "def parse_labeled_fasta(fasta_path):\n",
    "    data = []\n",
    "    for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "        seq = str(record.seq)\n",
    "        if record.id.startswith(\"Viru\"):\n",
    "            label = \"Viru\"\n",
    "        elif record.id.startswith(\"Temp\"):\n",
    "            label = \"Temp\"\n",
    "        elif record.id.startswith(\"Bact\"):\n",
    "            label = \"Bact\"\n",
    "        else:\n",
    "            continue  # Skip unknown\n",
    "        data.append({\"sequence\": seq, \"label\": label})\n",
    "    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7369e1e8-61c5-475f-8a92-4b657174fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evo import Evo\n",
    "\n",
    "evo_model = Evo('evo-1-131k-base')\n",
    "model, tokenizer = evo_model.model, evo_model.tokenizer\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd3ed0-bafd-4826-89d7-47d201ba3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GenomicDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, label2id):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.data[idx][\"sequence\"]\n",
    "        label = self.label2id[self.data[idx][\"label\"]]\n",
    "        tokens = self.tokenizer(seq, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": tokens[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokens[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3e354e-dfa4-4860-b5a3-7c88ce2c4a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvoClassifier(nn.Module):\n",
    "    def __init__(self, base_model_name, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(base_model_name)\n",
    "        self.classifier = nn.Linear(self.encoder.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state[:, 0]  # CLS token\n",
    "        return self.classifier(pooled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf6d267-81f4-45c7-aa68-3324de7d390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c62ae8-b244-4efb-ba72-c02d40126cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Evo tokenizer and model\n",
    "model_name = \"evo-1-131k-base\"  # Update if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = EvoClassifier(base_model_name=model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load and split data\n",
    "all_data = parse_labeled_fasta(\"datasets/fusion_sequences.fasta\")\n",
    "random.shuffle(all_data)\n",
    "split_idx = int(0.8 * len(all_data))\n",
    "train_data = all_data[:split_idx]\n",
    "val_data = all_data[split_idx:]\n",
    "\n",
    "train_dataset = GenomicDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):\n",
    "    loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (evo)",
   "language": "python",
   "name": "evo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
