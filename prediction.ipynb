{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9735635-9f13-4c53-b651-f5a786a4e3b7",
   "metadata": {},
   "source": [
    "`Viru`, `Temp`, `Bact` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c60ca55-5e77-43ac-90ca-2def1adc4370",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f280f0be-b2f7-4fee-92f9-11a2f07a9349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/project-gamma/evo_env/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e8d81e-07b5-4908-9c52-1905cf43110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e48a41c-c43c-45e1-b8d0-dd9564921371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from evo import Evo\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import AdamW\n",
    "from torchinfo import summary\n",
    "import gc\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdfd7163-7e00-4bb5-bec0-7f0ae227b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_labeled_fasta(fasta_path):\n",
    "    data = []\n",
    "    i=0\n",
    "    for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "        if i >= 1000:\n",
    "            break\n",
    "        seq = str(record.seq)\n",
    "        if record.id.startswith(\"Viru\"):\n",
    "            label = \"Viru\"\n",
    "        elif record.id.startswith(\"Temp\"):\n",
    "            label = \"Temp\"\n",
    "        elif record.id.startswith(\"Bact\"):\n",
    "            label = \"Bact\"\n",
    "        else:\n",
    "            continue  # Skip unknown\n",
    "        data.append({\"sequence\": seq, \"label\": label})\n",
    "        i += 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79cd3ed0-bafd-4826-89d7-47d201ba3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenomicDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, label2id, max_length=4096):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_length = max_length\n",
    "        self.pad_token = tokenizer.pad_token if hasattr(tokenizer, 'pad_token') else 'N'  # fallback if undefined\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.data[idx][\"sequence\"]\n",
    "        seq = seq[:self.max_length]\n",
    "        label = self.label2id[self.data[idx][\"label\"]]\n",
    "\n",
    "        # Truncate or pad sequence BEFORE tokenization\n",
    "        # if len(seq) < self.max_length:\n",
    "        #     seq += self.pad_token * (self.max_length - len(seq))\n",
    "        # else:\n",
    "        #     seq = seq[:self.max_length]\n",
    "\n",
    "        # Now tokenize the sequence\n",
    "        tokens = self.tokenizer.tokenize(seq)\n",
    "\n",
    "        input_ids = torch.tensor(tokens, dtype=torch.long)\n",
    "        attention_mask = (input_ids != self.tokenizer.pad_id).long()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a3e354e-dfa4-4860-b5a3-7c88ce2c4a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvoClassifier(nn.Module):\n",
    "    def __init__(self, encoder, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # Grab the true hidden dim\n",
    "        cfg = encoder.config\n",
    "        hidden_size = getattr(cfg, \"d_model\", None) or getattr(cfg, \"vocab_size\", None)\n",
    "        assert hidden_size == 512\n",
    "        if hidden_size is None:\n",
    "            raise ValueError(f\"No d_model or hidden_size in config: {cfg}\")\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            x_out, _ = self.encoder(input_ids, None, attention_mask)\n",
    "            pooled = x_out.mean(1).to(torch.float32)\n",
    "            #print(pooled.shape)\n",
    "            #print(pooled)\n",
    "            # outputs = self.encoder(input_ids, attention_mask)\n",
    "            # pooled = outputs.last_hidden_state[:, 0]  # CLS token\n",
    "        return self.classifier(pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaaaeb4-0a5f-4711-be21-d0874e63751d",
   "metadata": {},
   "source": [
    "### Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1ff558d-7e77-4fae-a59b-39ad8baa3001",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e6c6f337894975a2113cb85f998090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StripedHyena(\n",
       "  (embedding_layer): VocabParallelEmbedding(512, 4096)\n",
       "  (norm): RMSNorm()\n",
       "  (unembed): VocabParallelEmbedding(512, 4096)\n",
       "  (blocks): ModuleList(\n",
       "    (0-7): 8 x ParallelGatedConvBlock(\n",
       "      (pre_norm): RMSNorm()\n",
       "      (post_norm): RMSNorm()\n",
       "      (filter): ParallelHyenaFilter()\n",
       "      (projections): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "      (out_filter_dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (mlp): ParallelGatedMLP(\n",
       "        (l1): Linear(in_features=4096, out_features=10928, bias=False)\n",
       "        (l2): Linear(in_features=4096, out_features=10928, bias=False)\n",
       "        (l3): Linear(in_features=10928, out_features=4096, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (8): AttentionBlock(\n",
       "      (pre_norm): RMSNorm()\n",
       "      (post_norm): RMSNorm()\n",
       "      (inner_mha_cls): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "        (inner_attn): FlashSelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): FlashCrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      )\n",
       "      (mlp): ParallelGatedMLP(\n",
       "        (l1): Linear(in_features=4096, out_features=10928, bias=False)\n",
       "        (l2): Linear(in_features=4096, out_features=10928, bias=False)\n",
       "        (l3): Linear(in_features=10928, out_features=4096, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (9-15): 7 x ParallelGatedConvBlock(\n",
       "      (pre_norm): RMSNorm()\n",
       "      (post_norm): RMSNorm()\n",
       "      (filter): ParallelHyenaFilter()\n",
       "      (projections): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "      (out_filter_dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (mlp): ParallelGatedMLP(\n",
       "        (l1): Linear(in_features=4096, out_features=10928, bias=False)\n",
       "        (l2): Linear(in_features=4096, out_features=10928, bias=False)\n",
       "        (l3): Linear(in_features=10928, out_features=4096, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (16): AttentionBlock(\n",
       "      (pre_norm): RMSNorm()\n",
       "      (post_norm): RMSNorm()\n",
       "      (inner_mha_cls): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "        (inner_attn): FlashSelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): FlashCrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      )\n",
       "      (mlp): ParallelGatedMLP(\n",
       "        (l1): Linear(in_features=4096, out_features=10928, bias=False)\n",
       "        (l2): Linear(in_features=4096, out_features=10928, bias=False)\n",
       "        (l3): Linear(in_features=10928, out_features=4096, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (17-23): 7 x ParallelGatedConvBlock(\n",
       "      (pre_norm): RMSNorm()\n",
       "      (post_norm): RMSNorm()\n",
       "      (filter): ParallelHyenaFilter()\n",
       "      (projections): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "      (out_filter_dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (mlp): ParallelGatedMLP(\n",
       "        (l1): Linear(in_features=4096, out_features=10928, bias=False)\n",
       "        (l2): Linear(in_features=4096, out_features=10928, bias=False)\n",
       "        (l3): Linear(in_features=10928, out_features=4096, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (24): AttentionBlock(\n",
       "      (pre_norm): RMSNorm()\n",
       "      (post_norm): RMSNorm()\n",
       "      (inner_mha_cls): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "        (inner_attn): FlashSelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): FlashCrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      )\n",
       "      (mlp): ParallelGatedMLP(\n",
       "        (l1): Linear(in_features=4096, out_features=10928, bias=False)\n",
       "        (l2): Linear(in_features=4096, out_features=10928, bias=False)\n",
       "        (l3): Linear(in_features=10928, out_features=4096, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (25-31): 7 x ParallelGatedConvBlock(\n",
       "      (pre_norm): RMSNorm()\n",
       "      (post_norm): RMSNorm()\n",
       "      (filter): ParallelHyenaFilter()\n",
       "      (projections): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "      (out_filter_dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (mlp): ParallelGatedMLP(\n",
       "        (l1): Linear(in_features=4096, out_features=10928, bias=False)\n",
       "        (l2): Linear(in_features=4096, out_features=10928, bias=False)\n",
       "        (l3): Linear(in_features=10928, out_features=4096, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evo_model = Evo('evo-1.5-8k-base')\n",
    "model, tokenizer = evo_model.model, evo_model.tokenizer\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d7b2b2-8bc8-4e93-9b60-95b1cdb30fd2",
   "metadata": {},
   "source": [
    "### Load dataset and split train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeefe0c4-b5d4-4810-89bc-47b54d35a252",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\"Viru\": 0, \"Temp\": 1, \"Bact\": 2}\n",
    "all_data = parse_labeled_fasta(\"datasets/fusion_sequences_shuffled.fasta\")\n",
    "random.shuffle(all_data)\n",
    "split_idx = int(0.8 * len(all_data))\n",
    "train_data = all_data[:split_idx]\n",
    "val_data = all_data[split_idx:]\n",
    "\n",
    "train_dataset = GenomicDataset(train_data, tokenizer, label2id, max_length=4096)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "val_dataset = GenomicDataset(val_data, tokenizer, label2id, max_length=4096)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac22d3c5-0396-4ce5-badf-0face526f173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([65, 71, 67,  ..., 67, 84, 84]), 'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1]), 'label': tensor(1)}\n",
      "torch.Size([4096]) torch.Size([1, 4096, 512])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "x = train_dataset[0]['input_ids']\n",
    "with torch.no_grad():\n",
    "    y = model(x.unsqueeze(0).to(device))\n",
    "print(x.shape, y[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50316460-8c5d-4c77-aec6-c168be26ce3a",
   "metadata": {},
   "source": [
    "### Setup hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b836e1-3298-44ee-a42a-f3329d0d91b0",
   "metadata": {},
   "source": [
    "### Training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4919c180-5242-411e-bf5a-05aa59c5165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = EvoClassifier(model, num_classes=3).to(device)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = AdamW(m.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "478fef46-4011-4563-84aa-16096e038e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "EvoClassifier                                           --\n",
       "├─StripedHyena: 1-1                                     --\n",
       "│    └─VocabParallelEmbedding: 2-1                      2,097,152\n",
       "│    └─RMSNorm: 2-2                                     4,096\n",
       "│    └─VocabParallelEmbedding: 2-3                      (recursive)\n",
       "│    └─ModuleList: 2-4                                  --\n",
       "│    │    └─ParallelGatedConvBlock: 3-1                 201,601,024\n",
       "│    │    └─ParallelGatedConvBlock: 3-2                 201,601,024\n",
       "│    │    └─ParallelGatedConvBlock: 3-3                 201,601,024\n",
       "│    │    └─ParallelGatedConvBlock: 3-4                 201,601,024\n",
       "│    │    └─ParallelGatedConvBlock: 3-5                 201,601,024\n",
       "│    │    └─ParallelGatedConvBlock: 3-6                 201,601,024\n",
       "│    │    └─ParallelGatedConvBlock: 3-7                 201,601,024\n",
       "│    │    └─ParallelGatedConvBlock: 3-8                 201,601,024\n",
       "│    │    └─AttentionBlock: 3-9                         201,416,704\n",
       "│    │    └─ParallelGatedConvBlock: 3-10                201,601,024\n",
       "│    │    └─ParallelGatedConvBlock: 3-11                201,601,024\n",
       "│    │    └─ParallelGatedConvBlock: 3-12                201,601,024\n",
       "│    │    └─ParallelGatedConvBlock: 3-13                201,601,024\n",
       "│    │    └─ParallelGatedConvBlock: 3-14                201,601,024\n",
       "│    │    └─ParallelGatedConvBlock: 3-15                201,601,024\n",
       "│    │    └─ParallelGatedConvBlock: 3-16                201,601,024\n",
       "│    │    └─AttentionBlock: 3-17                        201,416,704\n",
       "│    │    └─ParallelGatedConvBlock: 3-18                201,601,024\n",
       "│    │    └─ParallelGatedConvBlock: 3-19                201,601,024\n",
       "│    │    └─ParallelGatedConvBlock: 3-20                201,601,024\n",
       "│    │    └─ParallelGatedConvBlock: 3-21                201,601,024\n",
       "│    │    └─ParallelGatedConvBlock: 3-22                201,601,024\n",
       "│    │    └─ParallelGatedConvBlock: 3-23                201,601,024\n",
       "│    │    └─ParallelGatedConvBlock: 3-24                201,601,024\n",
       "│    │    └─AttentionBlock: 3-25                        201,416,704\n",
       "│    │    └─ParallelGatedConvBlock: 3-26                201,601,024\n",
       "│    │    └─ParallelGatedConvBlock: 3-27                201,601,024\n",
       "│    │    └─ParallelGatedConvBlock: 3-28                201,601,024\n",
       "│    │    └─ParallelGatedConvBlock: 3-29                201,601,024\n",
       "│    │    └─ParallelGatedConvBlock: 3-30                201,601,024\n",
       "│    │    └─ParallelGatedConvBlock: 3-31                201,601,024\n",
       "│    │    └─ParallelGatedConvBlock: 3-32                201,601,024\n",
       "├─Linear: 1-2                                           1,539\n",
       "================================================================================\n",
       "Total params: 6,452,782,595\n",
       "Trainable params: 6,452,782,595\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c60959e-a2de-40ba-95b7-41a550bacdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1 — Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Train Loss: 0.7620\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4722 | Accuracy: 0.8800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# del model\n",
    "# gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# for param in m.encoder.parameters():\n",
    "#    param.requires_grad = False\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    m.train()\n",
    "    train_loss = 0.0\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs} — Training\")\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Train\", leave=False):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        padding_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        #with autocast():\n",
    "        logits = m(input_ids, attention_mask=padding_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1} | Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Phase validation\n",
    "    m.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    val_loss = 0.0\n",
    "    print(\"Validating...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Val\", leave=False):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            padding_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            #with autocast():\n",
    "            logits = m(input_ids, attention_mask=padding_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f} | Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfc23ee-63b8-4d1c-ba78-d32155a9afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddca8035-cb98-4880-ace7-c4fdb8b79d87",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Build DataFrames\u001b[39;00m\n\u001b[1;32m      5\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(train_data)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Build DataFrames\n",
    "train_df = pd.DataFrame(train_data)\n",
    "val_df = pd.DataFrame(val_data)\n",
    "\n",
    "\n",
    "def label_distribution(df, split_name):\n",
    "    counts = df['label'].value_counts().sort_index()\n",
    "    percents = (counts / len(df) * 100).round(2)\n",
    "    dist = pd.DataFrame({\n",
    "        'count': counts,\n",
    "        'percent': percents\n",
    "    })\n",
    "    dist.index.name = 'label'\n",
    "    dist.reset_index(inplace=True)\n",
    "    dist['split'] = split_name\n",
    "    return dist[['split', 'label', 'count', 'percent']]\n",
    "\n",
    "\n",
    "# Compute\n",
    "train_dist = label_distribution(train_df, 'train')\n",
    "val_dist = label_distribution(val_df,   'val')\n",
    "\n",
    "# Combine and display\n",
    "dist = pd.concat([train_dist, val_dist], ignore_index=True)\n",
    "print(dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62c0dc86-0878-4839-ab8b-e4e15d79c8c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m val_counts   \u001b[38;5;241m=\u001b[39m val_dist\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m labels \u001b[38;5;241m=\u001b[39m train_counts\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m----> 5\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(labels))\n\u001b[1;32m      6\u001b[0m width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.35\u001b[39m\n\u001b[1;32m      8\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "train_counts = train_dist.set_index('label')['count']\n",
    "val_counts   = val_dist.set_index('label')['count']\n",
    "\n",
    "labels = train_counts.index.tolist()\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# plot train and val bars side by side\n",
    "ax.bar(x - width/2, train_counts.values, width, label='train')\n",
    "ax.bar(x + width/2, val_counts.values, width, label='val')\n",
    "\n",
    "ax.set_xlabel('Label')\n",
    "ax.set_ylabel('Number of examples')\n",
    "ax.set_title('Label Distribution in Train vs Validation')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b389e16-7b4c-4d10-9425-6bd91a836a22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (evo_env)",
   "language": "python",
   "name": "evo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
