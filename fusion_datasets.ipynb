{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c198b5-d9c4-4f96-bc18-7822ed9ca2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n~/datasets/...\\n\\n\\nphage_fragmented/\\nx4\\n    - Dataset-1_temperate.fasta\\n\\n        => >Temp_gi|149|ref|NC_013055| Burkholderia phage KS9 linear:0-12169:1\\n            => >Temp\\n    idem pour viru\\n\\n    - Dataset-1_virulent.fasta\\n    - Dataset-2_temperate.fasta\\n    - Dataset-2_virulent.fasta\\n    \\nrefseq_simulated_metagenome/\\n    - host_chr_pvog_fragments.fasta\\n       => BACT\\n\\n\\n>Viru\\nACTG\\n\\n>Temp\\nACTG\\n\\n>Bact\\nACTG\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "~/datasets/...\n",
    "\n",
    "\n",
    "phage_fragmented/\n",
    "x4\n",
    "    - Dataset-1_temperate.fasta\n",
    "\n",
    "        => >Temp_gi|149|ref|NC_013055| Burkholderia phage KS9 linear:0-12169:1\n",
    "            => >Temp\n",
    "    idem pour viru\n",
    "\n",
    "    - Dataset-1_virulent.fasta\n",
    "    - Dataset-2_temperate.fasta\n",
    "    - Dataset-2_virulent.fasta\n",
    "    \n",
    "refseq_simulated_metagenome/\n",
    "    - host_chr_pvog_fragments.fasta\n",
    "       => BACT\n",
    "\n",
    "\n",
    ">Viru\n",
    "ACTG\n",
    "\n",
    ">Temp\n",
    "ACTG\n",
    "\n",
    ">Bact\n",
    "ACTG\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f45c7190-0241-46bf-8695-47c97ff41f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les séquences ont été fusionnées et enregistrées dans datasets/fusion_sequences.fasta\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Chemins vers les dossiers contenant les fichiers .fasta\n",
    "phage_dir = 'datasets/phage_fragmented/'\n",
    "refseq_dir = 'datasets/refseq_simulated_metagenome/'\n",
    "\n",
    "# Liste des fichiers à traiter\n",
    "fichiers = [\n",
    "    'Dataset-2_temperate_fragmented.fasta',\n",
    "    'Dataset-1_virulent_fragmented.fasta',\n",
    "    'Dataset-2_virulent_fragmented.fasta',\n",
    "    'Dataset-1_temperate_fragmented.fasta',\n",
    "    'host_chr_pvog_fragments.fasta'\n",
    "]\n",
    "\n",
    "# Chemin du fichier de sortie (dans le dossier datasets)\n",
    "output_file = 'datasets/fusion_sequences.fasta'\n",
    "\n",
    "# Créer le dossier 'datasets' si nécessaire\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "# Fonction pour extraire et modifier l'en-tête\n",
    "def modifier_header(header):\n",
    "    # Extraire la première colonne avant le premier \"|\"\n",
    "    first_column = header.split('|')[0]\n",
    "    if first_column.startswith(\">Viru\"):\n",
    "        return \">Viru\"\n",
    "    elif first_column.startswith(\">Temp\"):\n",
    "        return \">Temp\"\n",
    "    else:\n",
    "        return \">Bact\"\n",
    "\n",
    "# Fonction pour lire et fusionner les séquences\n",
    "def fusionner_sequences():\n",
    "    with open(output_file, 'w') as output:\n",
    "        # Traitement des fichiers du dossier phage_fragmented\n",
    "        for filename in fichiers:\n",
    "            file_path = None\n",
    "            if filename in os.listdir(phage_dir):\n",
    "                file_path = os.path.join(phage_dir, filename)\n",
    "            elif filename in os.listdir(refseq_dir):\n",
    "                file_path = os.path.join(refseq_dir, filename)\n",
    "\n",
    "            if file_path:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    header = None\n",
    "                    sequence = []\n",
    "                    for line in file:\n",
    "                        line = line.strip()\n",
    "                        if line.startswith('>'):\n",
    "                            # Si on a déjà une séquence, on l'écrit dans le fichier\n",
    "                            if header:\n",
    "                                output.write(f\"{header}\\n\")\n",
    "                                output.write(''.join(sequence) + '\\n')\n",
    "                            # Nouveau header\n",
    "                            header = modifier_header(line)\n",
    "                            sequence = []\n",
    "                        else:\n",
    "                            sequence.append(line)\n",
    "                    # Ajouter la dernière séquence lue\n",
    "                    if header:\n",
    "                        output.write(f\"{header}\\n\")\n",
    "                        output.write(''.join(sequence) + '\\n')\n",
    "\n",
    "# Exécution de la fusion\n",
    "fusionner_sequences()\n",
    "print(f\"Les séquences ont été fusionnées et enregistrées dans {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59cf0bb2-641e-4494-b162-194bbb1cc853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viru : 11565\n",
      "Temp : 2769\n",
      "Bact : 104003\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour compter les occurrences de Bact, Temp, et Viru dans le fichier\n",
    "def compter_occurrences(fichier):\n",
    "    counts = {'Bact': 0, 'Temp': 0, 'Viru': 0}\n",
    "    \n",
    "    with open(fichier, 'r') as file:\n",
    "        for line in file:\n",
    "            # Vérifier si la ligne est un en-tête\n",
    "            if line.startswith('>'):\n",
    "                # Vérifier le début de l'en-tête\n",
    "                if line.startswith('>Viru'):\n",
    "                    counts['Viru'] += 1\n",
    "                elif line.startswith('>Temp'):\n",
    "                    counts['Temp'] += 1\n",
    "                else:\n",
    "                    counts['Bact'] += 1\n",
    "    \n",
    "    return counts\n",
    "\n",
    "# Appeler la fonction pour compter les occurrences dans le fichier fusionné\n",
    "resultats = compter_occurrences('datasets/fusion_sequences.fasta')\n",
    "\n",
    "# Affichage des résultats\n",
    "print(f\"Viru : {resultats['Viru']}\")\n",
    "print(f\"Temp : {resultats['Temp']}\")\n",
    "print(f\"Bact : {resultats['Bact']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5827a1d7-46a4-43eb-b1eb-2170bd565d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour afficher les 50 premières lignes du fichier fusionné (en-tête + séquences)\n",
    "def afficher_head(fichier, n_lignes=50):\n",
    "    with open(fichier, 'r') as file:\n",
    "        count = 0\n",
    "        for line in file:\n",
    "            if count < n_lignes:\n",
    "                print(line.strip())  # Affiche chaque ligne sans sauter de ligne supplémentaire\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "# Appeler la fonction pour afficher les 50 premières lignes du fichier fusionné\n",
    "afficher_head('datasets/fusion_sequences.fasta', n_lignes=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de0706c4-b56f-44c9-bc31-26fd8df4567f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e5c5431d9445ec8677c3351a1b3010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des séquences: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type avant conversion: torch.uint8\n",
      "Type après conversion: torch.int64\n",
      "Type avant conversion: torch.uint8\n",
      "Type après conversion: torch.int64\n",
      "Type avant conversion: torch.uint8\n",
      "Type après conversion: torch.int64\n",
      "Type avant conversion: torch.uint8\n",
      "Type après conversion: torch.int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des séquences: 0it [00:13, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.17 GiB. GPU 0 has a total capacity of 21.98 GiB of which 1.89 GiB is free. Including non-PyTorch memory, this process has 20.07 GiB memory in use. Of the allocated memory 16.07 GiB is allocated by PyTorch, and 3.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Traitement des séquences par batches\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(load_sequences_in_batches(fichier_fasta, batch_size\u001b[38;5;241m=\u001b[39mbatch_size), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraitement des séquences\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Générer les embeddings pour chaque batch de séquences\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Ajouter les embeddings du batch à la liste\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     all_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n",
      "Cell \u001b[0;32mIn[5], line 46\u001b[0m, in \u001b[0;36mgenerate_embeddings\u001b[0;34m(sequences, model, tokenizer, device)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Passer au modèle pour obtenir les embeddings\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 46\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Ici, `outputs` est un tuple, donc nous accédons au premier élément\u001b[39;00m\n\u001b[1;32m     49\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Récupère la première partie du tuple (les embeddings)\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/project-gamma/evo_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/project-gamma/evo_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/project-gamma/evo_env/lib/python3.10/site-packages/stripedhyena/model.py:367\u001b[0m, in \u001b[0;36mStripedHyena.forward\u001b[0;34m(self, x, inference_params_dict, padding_mask)\u001b[0m\n\u001b[1;32m    362\u001b[0m     x, inference_params_dict_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstateful_forward(\n\u001b[1;32m    363\u001b[0m         x,\n\u001b[1;32m    364\u001b[0m         inference_params_dict\u001b[38;5;241m=\u001b[39minference_params_dict,\n\u001b[1;32m    365\u001b[0m     )\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 367\u001b[0m     x, inference_params_dict_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m    370\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munembed\u001b[38;5;241m.\u001b[39munembed(x)\n",
      "File \u001b[0;32m~/SageMaker/project-gamma/evo_env/lib/python3.10/site-packages/stripedhyena/model.py:386\u001b[0m, in \u001b[0;36mStripedHyena.stateless_forward\u001b[0;34m(self, x, padding_mask)\u001b[0m\n\u001b[1;32m    383\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m padding_mask[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[0;32m--> 386\u001b[0m     x, _ \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minference_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/project-gamma/evo_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/project-gamma/evo_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/project-gamma/evo_env/lib/python3.10/site-packages/stripedhyena/model.py:312\u001b[0m, in \u001b[0;36mParallelGatedConvBlock.forward\u001b[0;34m(self, u, inference_params, padding_mask, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(padding_mask) \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:  \u001b[38;5;66;03m# guard against bias\u001b[39;00m\n\u001b[1;32m    310\u001b[0m     z \u001b[38;5;241m=\u001b[39m z \u001b[38;5;241m*\u001b[39m padding_mask[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m--> 312\u001b[0m z, inference_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minference_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m z_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_filter_dense(z) \u001b[38;5;241m+\u001b[39m u\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(padding_mask) \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:  \u001b[38;5;66;03m# guard against bias\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/project-gamma/evo_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/project-gamma/evo_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/project-gamma/evo_env/lib/python3.10/site-packages/stripedhyena/model.py:161\u001b[0m, in \u001b[0;36mParallelHyenaFilter.forward\u001b[0;34m(self, u, inference_params, padding_mask, *args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequential_forward(u, inference_params)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minference_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/project-gamma/evo_env/lib/python3.10/site-packages/stripedhyena/model.py:179\u001b[0m, in \u001b[0;36mParallelHyenaFilter.parallel_forward\u001b[0;34m(self, u, inference_params, padding_mask)\u001b[0m\n\u001b[1;32m    176\u001b[0m     inference_params\u001b[38;5;241m.\u001b[39mfir_state_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idx] \u001b[38;5;241m=\u001b[39m fir_state\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     h, filter_dtype, poles, residues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh\n",
      "File \u001b[0;32m~/SageMaker/project-gamma/evo_env/lib/python3.10/site-packages/stripedhyena/model.py:273\u001b[0m, in \u001b[0;36mParallelHyenaFilter.compute_filter\u001b[0;34m(self, L, device)\u001b[0m\n\u001b[1;32m    268\u001b[0m filter_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m    269\u001b[0m residues, log_poles \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    270\u001b[0m     torch\u001b[38;5;241m.\u001b[39mview_as_complex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidues\u001b[38;5;241m.\u001b[39mto(filter_dtype)),\n\u001b[1;32m    271\u001b[0m     torch\u001b[38;5;241m.\u001b[39mview_as_complex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoles\u001b[38;5;241m.\u001b[39mto(filter_dtype))\u001b[38;5;241m.\u001b[39mlog(),\n\u001b[1;32m    272\u001b[0m )\n\u001b[0;32m--> 273\u001b[0m h \u001b[38;5;241m=\u001b[39m (residues \u001b[38;5;241m*\u001b[39m \u001b[43m(\u001b[49m\u001b[43mlog_poles\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mreal\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h, filter_dtype, log_poles, residues\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.17 GiB. GPU 0 has a total capacity of 21.98 GiB of which 1.89 GiB is free. Including non-PyTorch memory, this process has 20.07 GiB memory in use. Of the allocated memory 16.07 GiB is allocated by PyTorch, and 3.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "import torch\n",
    "from evo import Evo\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Fonction pour lire les séquences du fichier et les diviser en batches\n",
    "def load_sequences_in_batches(fichier, batch_size=64):\n",
    "    sequences = []\n",
    "    \n",
    "    # Lire le fichier fasta\n",
    "    for record in SeqIO.parse(fichier, \"fasta\"):\n",
    "        sequences.append(str(record.seq))\n",
    "    \n",
    "    # Diviser les séquences en batches\n",
    "    for i in range(0, len(sequences), batch_size):\n",
    "        yield sequences[i:i + batch_size]\n",
    "\n",
    "# Créer un modèle Evo\n",
    "evo_model = Evo('evo-1-131k-base')\n",
    "model, tokenizer = evo_model.model, evo_model.tokenizer\n",
    "model.eval()\n",
    "\n",
    "# S'assurer que tout se fait sur le GPU\n",
    "device = torch.device(\"cuda:0\")  # Utilisation du GPU 0\n",
    "model.to(device)  # Déplacer le modèle sur le GPU\n",
    "\n",
    "# Fonction pour générer les embeddings pour un batch de séquences\n",
    "def generate_embeddings(sequences, model, tokenizer, device='cuda:0'):\n",
    "    embeddings = []\n",
    "    for seq in sequences:\n",
    "        # Convertir les séquences en indices et les envoyer sur le GPU\n",
    "        input_ids = torch.tensor(tokenizer.tokenize(seq)).unsqueeze(0).to(device)  # Ajouter la dimension du batch et envoyer sur le GPU\n",
    "\n",
    "        # Vérifier le type du tensor (affichage du type pour vérification)\n",
    "        print(\"Type avant conversion:\", input_ids.dtype)\n",
    "\n",
    "        # Si le type n'est pas long, le convertir en torch.long\n",
    "        if input_ids.dtype != torch.long:\n",
    "            input_ids = input_ids.long()\n",
    "\n",
    "        # Vérifier à nouveau après conversion\n",
    "        print(\"Type après conversion:\", input_ids.dtype)\n",
    "\n",
    "        # Passer au modèle pour obtenir les embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "\n",
    "        # Ici, `outputs` est un tuple, donc nous accédons au premier élément\n",
    "        last_hidden_state = outputs[0]  # Récupère la première partie du tuple (les embeddings)\n",
    "        \n",
    "        # Moyenne des embeddings pour chaque séquence (ici nous utilisons mean sur les tokens)\n",
    "        embedding = last_hidden_state.mean(dim=1)\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Lire les séquences par batches et générer les embeddings\n",
    "fichier_fasta = 'datasets/fusion_sequences.fasta'\n",
    "batch_size = 64  # Taille du batch augmentée pour plus de performance\n",
    "\n",
    "all_embeddings = []  # Liste pour stocker tous les embeddings\n",
    "\n",
    "# Traitement des séquences par batches\n",
    "for batch in tqdm(load_sequences_in_batches(fichier_fasta, batch_size=batch_size), desc=\"Traitement des séquences\"):\n",
    "    # Générer les embeddings pour chaque batch de séquences\n",
    "    embeddings = generate_embeddings(batch, model, tokenizer, device=device)\n",
    "    \n",
    "    # Ajouter les embeddings du batch à la liste\n",
    "    all_embeddings.extend(embeddings)\n",
    "\n",
    "# Afficher un exemple des embeddings générés\n",
    "print(f\"Nombre d'embeddings générés : {len(all_embeddings)}\")\n",
    "print(\"Exemple d'embedding (première séquence) : \", all_embeddings[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (evo)",
   "language": "python",
   "name": "evo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
